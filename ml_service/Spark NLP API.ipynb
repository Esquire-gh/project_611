{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422591f3-1a9a-48ab-a4d2-b50c99a80e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q spark-nlp==5.5.1 pyspark==3.5.3 Flask==3.1.0 pyngrok==7.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ca452-7585-4ab1-867c-1f228d28d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sparknlp\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703b04e-6a22-42ee-9d6f-bc45096e936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "class AspectSentiment(BaseModel):\n",
    "    aspect: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "\n",
    "class Review(BaseModel):\n",
    "    reviewer_name: str\n",
    "    review_title: str\n",
    "    rating: int\n",
    "    review_text: str\n",
    "    date: str\n",
    "    verified_purchase: bool\n",
    "    review_imgs: List[str]\n",
    "    sentiment: Optional[str] = None\n",
    "    sentiment_score: Optional[float] = None\n",
    "    aspect_sentiments: Optional[List[AspectSentiment]] = None\n",
    "\n",
    "class Topic(BaseModel):\n",
    "    name: str\n",
    "    topic_name_explanation: Optional[str] = None\n",
    "    top_terms: List[Tuple[str, float]]\n",
    "    reviews: List[Review]\n",
    "    topic_sentiment_distribution: Dict[str, float] = Field(default_factory=dict)\n",
    "\n",
    "class Cluster(BaseModel):\n",
    "    topics: List[Topic]\n",
    "    cluster_name: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170e684-9b18-48ef-a193-3264108a982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, spark, model_name='sentimentdl_use_imdb'):\n",
    "        self.model_name = model_name\n",
    "        self.spark = spark\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        documentAssembler = DocumentAssembler()\\\n",
    "            .setInputCol(\"text\")\\\n",
    "            .setOutputCol(\"document\")\n",
    "\n",
    "        use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "            .setInputCols([\"document\"])\\\n",
    "            .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "        sentimentdl = SentimentDLModel.pretrained(name=self.model_name, lang=\"en\")\\\n",
    "            .setInputCols([\"sentence_embeddings\"])\\\n",
    "            .setOutputCol(\"sentiment\")\n",
    "\n",
    "        return Pipeline(stages=[documentAssembler, use, sentimentdl])\n",
    "\n",
    "    def analyze(self, reviews: List[Review]) -> List[Review]:\n",
    "        \"\"\"Analyze sentiment for a list of reviews\"\"\"\n",
    "        reviews_text = [rev.review_text for rev in reviews]\n",
    "        sentiment_df = self.spark.createDataFrame(reviews_text, StringType()).toDF(\"text\")\n",
    "        sentiment_result = self.pipeline.fit(sentiment_df).transform(sentiment_df)\n",
    "\n",
    "        updated_reviews = []\n",
    "        for i, (review, row) in enumerate(zip(reviews, sentiment_result.select(\"document\", \"sentiment\").collect())):\n",
    "            # Create a new Review instance with updated sentiment\n",
    "            review_dict = review.model_dump()\n",
    "            review_dict[\"sentiment\"] = row[\"sentiment\"][0][\"result\"]\n",
    "            updated_reviews.append(Review.model_validate(review_dict))\n",
    "\n",
    "        return updated_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2e3fd-1ae7-4c78-9011-474d84f9c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectBasedSentimentAnalyzer:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol('text')\\\n",
    "            .setOutputCol('document')\n",
    "\n",
    "        sentence_detector = SentenceDetector() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('sentence')\n",
    "\n",
    "        tokenizer = Tokenizer()\\\n",
    "            .setInputCols(['sentence']) \\\n",
    "            .setOutputCol('token')\n",
    "\n",
    "        word_embeddings = WordEmbeddingsModel.pretrained(\"glove_6B_300\", \"xx\")\\\n",
    "            .setInputCols([\"document\", \"token\"])\\\n",
    "            .setOutputCol(\"embeddings\")\n",
    "            \n",
    "        ner_model = NerDLModel.pretrained(\"ner_aspect_based_sentiment\")\\\n",
    "            .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n",
    "            .setOutputCol(\"ner\")\n",
    "\n",
    "        ner_converter = NerConverter()\\\n",
    "            .setInputCols(['sentence', 'token', 'ner']) \\\n",
    "            .setOutputCol('ner_chunk')\n",
    "\n",
    "        # Add sentiment classifier for overall sentiment\n",
    "        use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "            .setInputCols([\"document\"])\\\n",
    "            .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "        sentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_imdb\", lang=\"en\")\\\n",
    "            .setInputCols([\"sentence_embeddings\"])\\\n",
    "            .setOutputCol(\"sentiment\")\n",
    "\n",
    "        return Pipeline(stages=[\n",
    "            document_assembler, \n",
    "            sentence_detector,\n",
    "            tokenizer,\n",
    "            word_embeddings,\n",
    "            ner_model,\n",
    "            ner_converter,\n",
    "            use,\n",
    "            sentimentdl\n",
    "        ])\n",
    "\n",
    "    def _extract_aspects(self, row):\n",
    "        \"\"\"Extract aspects and their associated sentiments from the NER results\"\"\"\n",
    "        aspects = []\n",
    "        if row.ner_chunk:\n",
    "            for chunk in row.ner_chunk:\n",
    "                # Parse the metadata to get sentiment and confidence\n",
    "                metadata = chunk.metadata\n",
    "                aspect = chunk.result\n",
    "                sentiment = metadata.get('sentiment', 'neutral')\n",
    "                confidence = float(metadata.get('probability', 0.0))  # Changed from 'confidence' to 'probability'\n",
    "                \n",
    "                aspects.append(AspectSentiment(\n",
    "                    aspect=aspect,\n",
    "                    sentiment=sentiment,\n",
    "                    confidence=confidence\n",
    "                ))\n",
    "        return aspects\n",
    "\n",
    "    def _extract_sentiment_score(self, sentiment_result):\n",
    "        \"\"\"Extract sentiment score from sentiment result\"\"\"\n",
    "        try:\n",
    "            # First try to get the score from probability field\n",
    "            if 'probability' in sentiment_result['metadata']:\n",
    "                return float(sentiment_result['metadata']['probability'])\n",
    "            # If not found, try to get the highest class probability\n",
    "            elif 'class_probability' in sentiment_result['metadata']:\n",
    "                return max(float(p) for p in sentiment_result['metadata']['class_probability'].split('|'))\n",
    "            # If neither exists, return a default score\n",
    "            return 1.0\n",
    "        except (KeyError, ValueError):\n",
    "            return 1.0\n",
    "\n",
    "    def analyze(self, reviews: List[Review]) -> List[Review]:\n",
    "        \"\"\"Analyze both overall sentiment and aspect-based sentiments for a list of reviews\"\"\"\n",
    "        reviews_text = [rev.review_text for rev in reviews]\n",
    "        sentiment_df = self.spark.createDataFrame(reviews_text, StringType()).toDF(\"text\")\n",
    "        \n",
    "        # Apply the pipeline\n",
    "        result = self.pipeline.fit(sentiment_df).transform(sentiment_df)\n",
    "        \n",
    "        updated_reviews = []\n",
    "        for i, (review, row) in enumerate(zip(\n",
    "            reviews, \n",
    "            result.select(\"document\", \"sentiment\", \"ner_chunk\").collect()\n",
    "        )):\n",
    "            # Create a new Review instance with updated sentiments\n",
    "            review_dict = review.model_dump()\n",
    "            \n",
    "            # Set overall sentiment\n",
    "            sentiment_result = row[\"sentiment\"][0]\n",
    "            review_dict[\"overall_sentiment\"] = sentiment_result[\"result\"]\n",
    "            review_dict[\"overall_sentiment_score\"] = self._extract_sentiment_score(sentiment_result)\n",
    "            \n",
    "            # Extract aspect-based sentiments\n",
    "            review_dict[\"aspect_sentiments\"] = self._extract_aspects(row)\n",
    "            \n",
    "            updated_reviews.append(Review.model_validate(review_dict))\n",
    "\n",
    "        return updated_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179815f-dff3-4371-b1a6-1e7d7a04a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModeler:\n",
    "    def __init__(self, spark, num_topics=5, vocab_size=1000):\n",
    "        self.num_topics = num_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        self.spark = spark\n",
    "        self.pipeline = self._create_pipeline()\n",
    "        self.model = None\n",
    "        self.vocabulary = None\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "        sentence = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentences\")\n",
    "        tokenizer = Tokenizer().setInputCols([\"sentences\"]).setOutputCol(\"tokens\")\n",
    "        normalizer = Normalizer().setInputCols([\"tokens\"]).setOutputCol(\"normalized\").setLowercase(True)\n",
    "        stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False)\n",
    "        lemmatizer = LemmatizerModel.pretrained().setInputCols([\"cleanTokens\"]).setOutputCol(\"lemmas\")\n",
    "        finisher = Finisher().setInputCols([\"lemmas\"]).setOutputCols([\"finished_lemmas\"]).setCleanAnnotations(False)\n",
    "        cv = CountVectorizer(inputCol=\"finished_lemmas\", outputCol=\"raw_features\", vocabSize=self.vocab_size, minDF=2.0)\n",
    "        idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "        lda = LDA(k=self.num_topics, maxIter=10, featuresCol=\"features\", optimizer=\"online\")\n",
    "\n",
    "        return Pipeline(stages=[document, sentence, tokenizer, normalizer, stopwords_cleaner, lemmatizer, finisher, cv, idf, lda])\n",
    "\n",
    "    def fit_transform(self, reviews: List[Review]) -> Cluster:\n",
    "        \"\"\"Fit the topic model and transform the reviews into topics\"\"\"\n",
    "        reviews_text = [review.review_text for review in reviews]\n",
    "        df = self.spark.createDataFrame([(text,) for text in reviews_text], [\"text\"])\n",
    "\n",
    "        self.model = self.pipeline.fit(df)\n",
    "        transformed = self.model.transform(df)\n",
    "\n",
    "        self.vocabulary = self.model.stages[7].vocabulary\n",
    "\n",
    "        return self._create_cluster(transformed, reviews)\n",
    "\n",
    "    def _calculate_sentiment_distribution(self, reviews: List[Review]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate sentiment distribution for a list of reviews\"\"\"\n",
    "        sentiments = [review.sentiment for review in reviews]\n",
    "        distribution = {}\n",
    "        total = len(sentiments)\n",
    "\n",
    "        for sentiment in set(sentiments):\n",
    "            if sentiment:\n",
    "                count = sentiments.count(sentiment)\n",
    "                distribution[sentiment] = count / total * 100 if total > 0 else 0\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def _create_cluster(self, transformed_df, reviews: List[Review]) -> Cluster:\n",
    "        \"\"\"Create a cluster from transformed data\"\"\"\n",
    "        topics = self.model.stages[-1].describeTopics()\n",
    "        topics_list = []\n",
    "\n",
    "        for topic_id, terms in enumerate(topics.collect()):\n",
    "            top_terms = [(self.vocabulary[term_id], float(weight))  # Convert numpy float to Python float\n",
    "                        for term_id, weight in zip(terms['termIndices'], terms['termWeights'])]\n",
    "            topic_name = f\"Topic {topic_id}\"\n",
    "            reviews_for_topic = []\n",
    "\n",
    "            for row in transformed_df.select(\"text\", \"topicDistribution\").collect():\n",
    "                if row[\"topicDistribution\"][topic_id] > 0.1:\n",
    "                    for review in reviews:\n",
    "                        if row.text == review.review_text:\n",
    "                            reviews_for_topic.append(review)\n",
    "\n",
    "            sentiment_distribution = self._calculate_sentiment_distribution(reviews_for_topic)\n",
    "\n",
    "            topics_list.append(Topic(\n",
    "                name=topic_name,\n",
    "                top_terms=top_terms,\n",
    "                reviews=reviews_for_topic,\n",
    "                topic_sentiment_distribution=sentiment_distribution\n",
    "            ))\n",
    "\n",
    "        return Cluster(topics=topics_list, cluster_name='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5b14d-6c90-4452-bb52-6d2293c3d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_reviews(raw_reviews: dict) -> List[Review]:\n",
    "    return [\n",
    "        Review(\n",
    "            reviewer_name=review['reviewer_name'],\n",
    "            review_title=review['review_title'],\n",
    "            rating=review['rating'],\n",
    "            review_text=review['review_text'],\n",
    "            date=review['date'],\n",
    "            verified_purchase=review['verified_perchase'],  # Note: fixed typo in field name\n",
    "            review_imgs=review['review_imgs']\n",
    "        )\n",
    "        for review in raw_reviews\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca591d-e334-4c94-a8b6-c8652d248704",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16742b-4e0e-4afd-b9ed-27757e57f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer(spark)\n",
    "# aspect_based_sentiment_analyser = AspectBasedSentimentAnalyzer(spark)\n",
    "topic_modeler = TopicModeler(spark, num_topics=5, vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2b63d-e594-4857-85e2-3c3064f52f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "\n",
    "class TopicNamingResponse(BaseModel):\n",
    "    topic_name: str\n",
    "    explanation: Optional[str] = None\n",
    "\n",
    "class OpenAITopicNamer:\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini-2024-07-18\"):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _create_prompt(self, top_terms: List[Tuple[str, float]]) -> str:\n",
    "        terms_str = \", \".join([f\"{term} ({weight:.3f})\" for term, weight in top_terms])\n",
    "\n",
    "        return f\"\"\"Given these weighted terms from a topic model: {terms_str}\n",
    "\n",
    "                  Please:\n",
    "                  1. Analyze these terms and their weights\n",
    "                  2. Identify the underlying theme or concept they represent\n",
    "                  3. Create a short (2-4 words), clear name for this topic\n",
    "                  4. Provide a brief explanation of why this name fits the terms\n",
    "\n",
    "                  Respond with a JSON object having these fields:\n",
    "                  - topic_name (string): A short, clear name for the topic\n",
    "                  - explanation (optional string): A brief explanation of why this name fits\"\"\"\n",
    "\n",
    "    def name_topic(self, top_terms: List[Tuple[str, float]], temperature: float = 0.3) -> TopicNamingResponse:\n",
    "        try:\n",
    "            prompt = self._create_prompt(top_terms)\n",
    "\n",
    "            response = self.client.beta.chat.completions.parse(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful AI that specializes in analyzing topic models and creating insightful topic names.\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                response_format=TopicNamingResponse,\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error naming topic: {str(e)}\")\n",
    "            return TopicNamingResponse(\n",
    "                topic_name=f\"Topic ({', '.join(term for term, _ in top_terms[:3])})\",\n",
    "                explanation=\"Fallback name using top terms due to naming service error\"\n",
    "            )\n",
    "\n",
    "    def update_cluster_topics(self, cluster: 'Cluster') -> 'Cluster':\n",
    "        try:\n",
    "            updated_topics = []\n",
    "\n",
    "            for topic in cluster.topics:\n",
    "                naming_response = self.name_topic(topic.top_terms)\n",
    "\n",
    "                topic_dict = topic.model_dump()\n",
    "                topic_dict['name'] = naming_response.topic_name\n",
    "                topic_dict['topic_name_explanation'] = naming_response.explanation\n",
    "                updated_topic = Topic.model_validate(topic_dict)\n",
    "                updated_topics.append(updated_topic)\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            cluster_dict = cluster.model_dump()\n",
    "            cluster_dict['topics'] = updated_topics\n",
    "            return Cluster.model_validate(cluster_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error updating cluster topics: {str(e)}\")\n",
    "            return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53927c6-8013-445c-9dcd-dfa090add235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, make_response\n",
    "from google.colab import userdata\n",
    "from pyngrok import ngrok\n",
    "\n",
    "PORT = 5000\n",
    "\n",
    "ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN'))\n",
    "public_url = ngrok.connect(PORT).public_url\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=['POST'])\n",
    "def main():\n",
    "  data = request.get_json()\n",
    "\n",
    "  if not data:\n",
    "    return jsonify({\"error\": \"No JSON data provided\"}), 400\n",
    "\n",
    "  reviews = process_raw_reviews(data)\n",
    "\n",
    "  reviews_with_sentiment = sentiment_analyzer.analyze(reviews)\n",
    "  # reviews_with_sentiment = aspect_based_sentiment_analyser.analyze(reviews)\n",
    "\n",
    "  # for review in reviews_with_sentiment:\n",
    "  #   for aspect in review.aspect_sentiments:\n",
    "  #     print(aspect.aspect)\n",
    "  #     print(aspect.sentiment)\n",
    "  #   print('-'*50)\n",
    "\n",
    "  # topic_modeler = TopicModeler(spark, num_topics=5, vocab_size=1000)\n",
    "  cluster = topic_modeler.fit_transform(reviews_with_sentiment)\n",
    "\n",
    "  topic_namer = OpenAITopicNamer(api_key=userdata.get('OPENAIAPI_SECRET_KEY'))\n",
    "  updated_cluster = topic_namer.update_cluster_topics(cluster)\n",
    "\n",
    "  return make_response(\n",
    "        updated_cluster.model_dump_json(),\n",
    "        200,\n",
    "        {'Content-Type': 'application/json'}\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  print(f\"Access the app on {public_url}\")\n",
    "  app.run(port=PORT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
